{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40462860-6cb9-4886-ab30-1260230c226f",
   "metadata": {},
   "source": [
    "# Resource\n",
    "[deepsphere (paper)](https://arxiv.org/abs/1810.12186)\n",
    "> [deepsphere-pytorch (github)](https://github.com/deepsphere/deepsphere-pytorch)\n",
    "\n",
    "[Cosmological Parameter Estimation and Inference using Deep Summaries (paper)](https://arxiv.org/abs/2107.09002)\n",
    "> [cosmo_estimators (github)](https://github.com/jafluri/cosmo_estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fc046-afc4-491c-bdf2-62d54001a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.utils import alias, deprecated_arg, export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a9f921-1c34-434a-b8a0-a99d440b1a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T16:12:45.301968Z",
     "iopub.status.busy": "2021-10-26T16:12:45.301107Z",
     "iopub.status.idle": "2021-10-26T16:12:45.310825Z",
     "shell.execute_reply": "2021-10-26T16:12:45.309631Z",
     "shell.execute_reply.started": "2021-10-26T16:12:45.301927Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Example script for running DeepSphere U-Net on reduced AR_TC dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from ignite.contrib.handlers.param_scheduler import create_lr_scheduler_with_warmup\n",
    "from ignite.contrib.handlers.tensorboard_logger import GradsHistHandler, OptimizerParamsHandler, OutputHandler, TensorboardLogger, WeightsHistHandler\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "from ignite.handlers import EarlyStopping, TerminateOnNan\n",
    "from ignite.metrics import EpochMetric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "from deepsphere.data.datasets.dataset import ARTCDataset\n",
    "from deepsphere.data.transforms.transforms import Normalize, Permute, ToTensor\n",
    "from deepsphere.models.spherical_unet.unet_model import SphericalUNet\n",
    "from deepsphere.utils.initialization import init_device\n",
    "from deepsphere.utils.parser import create_parser, parse_config\n",
    "from deepsphere.utils.stats_extractor import stats_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8952a-fbf9-4f2f-83e5-d3c85880d2e5",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb9870d8-4428-4aac-a532-df48ea87a938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T23:47:38.486710Z",
     "iopub.status.busy": "2021-10-26T23:47:38.485628Z",
     "iopub.status.idle": "2021-10-26T23:47:38.496289Z",
     "shell.execute_reply": "2021-10-26T23:47:38.495012Z",
     "shell.execute_reply.started": "2021-10-26T23:47:38.486646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from ignite.contrib.handlers.param_scheduler import create_lr_scheduler_with_warmup\n",
    "from ignite.contrib.handlers.tensorboard_logger import GradsHistHandler, OptimizerParamsHandler, OutputHandler, TensorboardLogger, WeightsHistHandler\n",
    "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
    "from ignite.handlers import EarlyStopping, TerminateOnNan\n",
    "from ignite.metrics import EpochMetric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "from deepsphere.data.transforms.transforms import Normalize, Permute, ToTensor\n",
    "from deepsphere.utils.initialization import init_dataset_temp, init_device, init_unet_temp\n",
    "from deepsphere.utils.parser import create_parser, parse_config\n",
    "from deepsphere.utils.stats_extractor import stats_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e3a031a-6fa5-4289-a3bf-339f07723eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T23:18:21.694816Z",
     "iopub.status.busy": "2021-10-26T23:18:21.693780Z",
     "iopub.status.idle": "2021-10-26T23:18:21.701763Z",
     "shell.execute_reply": "2021-10-26T23:18:21.700460Z",
     "shell.execute_reply.started": "2021-10-26T23:18:21.694753Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from deepsphere.utils.initialization import init_dataset_temp, init_device, init_unet_temp\n",
    "from deepsphere.utils.initialization import init_device\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "HOME = os.path.expandvars('$HOME')\n",
    "dataset_dir=HOME+'/code/shear2convergence/recon_kappa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9743be-13fb-4e8f-9d9d-5de5412a9158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T20:52:02.109952Z",
     "iopub.status.busy": "2021-10-26T20:52:02.109632Z",
     "iopub.status.idle": "2021-10-26T20:52:02.238345Z",
     "shell.execute_reply": "2021-10-26T20:52:02.237583Z",
     "shell.execute_reply.started": "2021-10-26T20:52:02.109877Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59719/4198767088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m997\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x1_{:03} = np.load(dataset_dir+'gamma1_w_noise/'+'g1_'+ '{:03}' +'.npy')\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x2_{:03} = np.load(dataset_dir+'gamma2_w_noise/'+'g2_'+ '{:03}' +'.npy')\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_{:03} = np.load(dataset_dir+'kappa/'+'kappa_'+ '{:03}' +'.npy')\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define global value\n",
    "def _exec(args):\n",
    "    return exec(args, globals())\n",
    "\n",
    "for i in range(997,1000):\n",
    "    exec(\"x1_{:03} = np.load(dataset_dir+'gamma1_w_noise/'+'g1_'+ '{:03}' +'.npy')\".format(i,i), globals())\n",
    "    exec(\"x2_{:03} = np.load(dataset_dir+'gamma2_w_noise/'+'g2_'+ '{:03}' +'.npy')\".format(i,i), globals())\n",
    "    exec(\"y_{:03} = np.load(dataset_dir+'kappa/'+'kappa_'+ '{:03}' +'.npy')\".format(i,i), globals())\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4db7ed89-5701-4c87-86de-21c2c5ac130a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T20:34:40.862497Z",
     "iopub.status.busy": "2021-10-26T20:34:40.861452Z",
     "iopub.status.idle": "2021-10-26T20:34:40.867680Z",
     "shell.execute_reply": "2021-10-26T20:34:40.866483Z",
     "shell.execute_reply.started": "2021-10-26T20:34:40.862440Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc96d91-159d-4e6a-bf11-17fc0f0ae2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T21:36:08.505002Z",
     "iopub.status.busy": "2021-10-26T21:36:08.504012Z",
     "iopub.status.idle": "2021-10-26T21:36:08.516850Z",
     "shell.execute_reply": "2021-10-26T21:36:08.515678Z",
     "shell.execute_reply.started": "2021-10-26T21:36:08.504940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, gamma1_dir, gamma2_dir, kappa_dir):\n",
    "        self.gamma1_dir = gamma1_dir\n",
    "        self.gamma2_dir = gamma2_dir\n",
    "        self.kappa_dir = kappa_dir\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        x1 = np.load(f\"{self.gamma1_dir}/g1_{index:03}.npy\")\n",
    "        x2 = np.load(f\"{self.gamma2_dir}/g2_{index:03}.npy\")\n",
    "        y = np.load(f\"{self.kappa_dir}/kappa_{index:03}.npy\")\n",
    "        x1 = torch.from_numpy(x1).float()\n",
    "        x2 = torch.from_numpy(x2).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        \n",
    "        x = torch.stack((x1,x2))\n",
    "        y = torch.stack((y,))\n",
    "        \n",
    "        sample = {'gamma': x, 'kappa': y}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        filenames = os.listdir(self.gamma1_dir)\n",
    "        res = 0\n",
    "        for name in filenames:\n",
    "            if '.npy' in name:\n",
    "                res+=1\n",
    "        return res\n",
    "\n",
    "gamma1_dir = dataset_dir+'gamma1_w_noise'\n",
    "gamma2_dir = dataset_dir+'gamma2_w_noise'\n",
    "kappa_dir = dataset_dir+'kappa'\n",
    "\n",
    "# # transform = tr_get_means_stdsansforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n",
    "dataset = MyDataset(gamma1_dir, gamma2_dir, kappa_dir)\n",
    "# dataloader = DataLoader(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "479cd8e2-1f71-4142-893b-d342ec3b1f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T21:48:33.413387Z",
     "iopub.status.busy": "2021-10-26T21:48:33.412516Z",
     "iopub.status.idle": "2021-10-26T21:48:33.582924Z",
     "shell.execute_reply": "2021-10-26T21:48:33.581170Z",
     "shell.execute_reply.started": "2021-10-26T21:48:33.413327Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': tensor([[ 0.0041,  0.0022,  0.0012,  ..., -0.0071, -0.0034, -0.0002],\n",
       "         [ 0.0207, -0.0167,  0.0117,  ...,  0.0044, -0.0048, -0.0019]]),\n",
       " 'kappa': tensor([[ 0.0034,  0.0043, -0.0054,  ..., -0.0086,  0.0011, -0.0017]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa562fb3-2f11-4932-aab1-f458f97091cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T22:15:55.966229Z",
     "iopub.status.busy": "2021-10-26T22:15:55.965013Z",
     "iopub.status.idle": "2021-10-26T22:16:03.247195Z",
     "shell.execute_reply": "2021-10-26T22:16:03.244944Z",
     "shell.execute_reply.started": "2021-10-26T22:15:55.966170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_means_stds(dataset, key: str):\n",
    "    channel, length = torch.Tensor(dataset[0][key]).shape\n",
    "    \n",
    "    summing = torch.zeros(channel)\n",
    "    square_summing = torch.zeros(channel)\n",
    "    total = 0\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx][key]\n",
    "        summing += torch.sum(sample, dim=1)\n",
    "        total += sample.shape[1]\n",
    "    means = torch.unsqueeze(summing / total, dim=1)\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx][key]\n",
    "        square_summing += torch.sum((sample - means) ** 2, dim=1)\n",
    "    stds = torch.sqrt(square_summing / (total - 1))\n",
    "    \n",
    "    return torch.squeeze(means, dim=1).numpy(), stds.numpy()\n",
    "\n",
    "for key in dataset[0].keys():\n",
    "    means, stds = _get_means_stds(dataset, key)\n",
    "    np.save(\"./tmp_save/means_\"+key+\".npy\", means)\n",
    "    np.save(\"./tmp_save/stds_\"+key+\".npy\", stds)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad57a1fe-3606-4716-b314-142bb55a6477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T23:33:52.619015Z",
     "iopub.status.busy": "2021-10-26T23:33:52.618507Z",
     "iopub.status.idle": "2021-10-26T23:33:52.636483Z",
     "shell.execute_reply": "2021-10-26T23:33:52.635238Z",
     "shell.execute_reply.started": "2021-10-26T23:33:52.618960Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader(gamma1_dir, gamma2_dir, kappa_dir,train_val_ratio,train_test_ratio):\n",
    "    # read data\n",
    "    dataset = MyDataset(gamma1_dir, gamma2_dir, kappa_dir)\n",
    "    \n",
    "    def _get_means_stds(dataset, key: str):\n",
    "        channel, length = torch.Tensor(dataset[0][key]).shape\n",
    "\n",
    "        summing = torch.zeros(channel)\n",
    "        square_summing = torch.zeros(channel)\n",
    "        total = 0\n",
    "\n",
    "        for idx in range(len(dataset)):\n",
    "            sample = dataset[idx][key]\n",
    "            summing += torch.sum(sample, dim=1)\n",
    "            total += sample.shape[1]\n",
    "        means = torch.unsqueeze(summing / total, dim=1)\n",
    "\n",
    "        for idx in range(len(dataset)):\n",
    "            sample = dataset[idx][key]\n",
    "            square_summing += torch.sum((sample - means) ** 2, dim=1)\n",
    "        stds = torch.sqrt(square_summing / (total - 1))\n",
    "\n",
    "        return torch.squeeze(means, dim=1).numpy(), stds.numpy()\n",
    "\n",
    "    for key in dataset[0].keys():\n",
    "        means, stds = _get_means_stds(dataset, key)\n",
    "        np.save(\"./tmp_save/means_\"+key+\".npy\", means)\n",
    "        np.save(\"./tmp_save/stds_\"+key+\".npy\", stds)\n",
    "        pass\n",
    "    means_gamma = np.load(\"./tmp_save/means_gamma.npy\")\n",
    "    means_kappa = np.load(\"./tmp_save/means_kappa.npy\")\n",
    "    stds_gamma = np.load(\"./tmp_save/stds_gamma.npy\")\n",
    "    stds_kappa = np.load(\"./tmp_save/stds_kappa.npy\")\n",
    "    transform_gamma = transforms.Compose([transforms.Normalize(mean=means_gamma, std=stds_gamma)])\n",
    "    transform_kappa = transforms.Compose([transforms.Normalize(mean=means_kappa, std=stds_kappa)])\n",
    "    \n",
    "    num_samples = len(dataset)\n",
    "    num_train_samples = int(round(num_samples * train_val_ratio))\n",
    "    num_test_samples = int(round(num_samples * train_test_ratio))\n",
    "    num_val_samples = num_samples - num_train_samples - num_test_samples\n",
    "    assert num_val_samples > 0\n",
    "    \n",
    "    splits = (num_train_samples, num_val_samples, num_test_samples)\n",
    "    train_subjects, val_subjects,  test_subjects = random_split(dataset, splits)\n",
    "    \n",
    "    dataloader_train = DataLoader(train_subjects, batch_size=32, shuffle=True, num_workers=10)\n",
    "    dataloader_val = DataLoader(train_subjects, batch_size=32, shuffle=True, num_workers=6)\n",
    "    dataloader_test = DataLoader(train_subjects, batch_size=32, shuffle=True, num_workers=6)\n",
    "    \n",
    "    return dataloader_train, dataloader_val, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eda329aa-c8c6-4c5c-8a3f-2c8b44dbaf67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T23:04:25.166149Z",
     "iopub.status.busy": "2021-10-26T23:04:25.165453Z",
     "iopub.status.idle": "2021-10-26T23:04:26.087553Z",
     "shell.execute_reply": "2021-10-26T23:04:26.085398Z",
     "shell.execute_reply.started": "2021-10-26T23:04:25.166111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepsphere.utils.initialization import init_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24712bcc-2f46-46e2-9d13-9ca901fa4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67717a72-9b3a-4c04-a500-6b426ae437f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T23:34:02.996923Z",
     "iopub.status.busy": "2021-10-26T23:34:02.995893Z",
     "iopub.status.idle": "2021-10-26T23:44:26.124813Z",
     "shell.execute_reply": "2021-10-26T23:44:26.122241Z",
     "shell.execute_reply.started": "2021-10-26T23:34:02.996875Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59719/2392577419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlength_per_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkappa_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'kappa_000.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "gamma1_dir = dataset_dir+'gamma1_w_noise/'\n",
    "gamma2_dir = dataset_dir+'gamma2_w_noise/'\n",
    "kappa_dir = dataset_dir+'kappa/'\n",
    "\n",
    "dataloader_train, dataloader_val, dataloader_test = prepare_dataloader(\n",
    "    gamma1_dir, gamma2_dir, kappa_dir,\n",
    "    train_val_ratio = 0.2,\n",
    "    train_test_ratio = 0.1\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "length_per_map = np.load(kappa_dir+'kappa_000.npy').shape[0]\n",
    "\n",
    "unet = SphericalUNet(\n",
    "    pooling_class = \"healpix\",\n",
    "    N = length_per_map,\n",
    "    depth = 4,\n",
    "    kernel_size = 3\n",
    ")\n",
    "\n",
    "unet, device = init_device(cpu, unet)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16edabd7-c7f5-4c74-8466-aa1433f2ed86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-26T19:24:44.635957Z",
     "iopub.status.busy": "2021-10-26T19:24:44.634856Z",
     "iopub.status.idle": "2021-10-26T19:24:44.643542Z",
     "shell.execute_reply": "2021-10-26T19:24:44.642353Z",
     "shell.execute_reply.started": "2021-10-26T19:24:44.635892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145728,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainer(engine, batch):\n",
    "    unet.train()\n",
    "    x, y = batch['gamma'], batch['kappa']\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    y_hat = unet(x)\n",
    "    \n",
    "    B, V, C = output.shape\n",
    "    B_labels, V_labels, C_labels = labels.shape\n",
    "    output = output.view(B * V, C)\n",
    "    labels = labels.view(B_labels * V_labels, C_labels).max(1)[1]\n",
    "    \n",
    "    loss = criterion(y_hat,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "writer = SummaryWriter(parser_args.tensorboard_path)\n",
    "\n",
    "engine_train = Engine(trainer)\n",
    "\n",
    "engine_validate = create_supervised_evaluator(\n",
    "    model=unet, metrics={\"AP\": EpochMetric(average_precision_compute_fn)}, device=device, output_transform=validate_output_transform\n",
    ")\n",
    "\n",
    "engine_train.add_event_handler(Events.EPOCH_STARTED, lambda x: print(\"Starting Epoch: {}\".format(x.state.epoch)))\n",
    "engine_train.add_event_handler(Events.ITERATION_COMPLETED, TerminateOnNan())\n",
    "\n",
    "@engine_train.on(Events.EPOCH_COMPLETED)\n",
    "def epoch_validation(engine):\n",
    "    \"\"\"Handler to run the validation engine at the end of the train engine's epoch.\n",
    "\n",
    "    Args:\n",
    "        engine (ignite.engine): train engine\n",
    "    \"\"\"\n",
    "    print(\"beginning validation epoch\")\n",
    "    engine_validate.run(dataloader_validation)\n",
    "\n",
    "reduce_lr_plateau = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=parser_args.reducelronplateau_mode,\n",
    "    factor=parser_args.reducelronplateau_factor,\n",
    "    patience=parser_args.reducelronplateau_patience,\n",
    ")\n",
    "\n",
    "@engine_validate.on(Events.EPOCH_COMPLETED)\n",
    "def update_reduce_on_plateau(engine):\n",
    "    \"\"\"Handler to reduce the learning rate on plateau at the end of the validation engine's epoch\n",
    "\n",
    "    Args:\n",
    "        engine (ignite.engine): validation engine\n",
    "    \"\"\"\n",
    "    ap = engine.state.metrics[\"AP\"]\n",
    "    mean_average_precision = np.mean(ap[1:])\n",
    "    reduce_lr_plateau.step(mean_average_precision)\n",
    "\n",
    "@engine_validate.on(Events.EPOCH_COMPLETED)\n",
    "def save_epoch_results(engine):\n",
    "    \"\"\"Handler to save the metrics at the end of the validation engine's epoch\n",
    "\n",
    "    Args:\n",
    "        engine (ignite.engine): validation engine\n",
    "    \"\"\"\n",
    "    ap = engine.state.metrics[\"AP\"]\n",
    "    mean_average_precision = np.mean(ap[1:])\n",
    "    print(\"Average precisions:\", ap)\n",
    "    print(\"mAP:\", mean_average_precision)\n",
    "    writer.add_scalars(\n",
    "        \"metrics\",\n",
    "        {\"mean average precision (AR+TC)\": mean_average_precision, \"AR average precision\": ap[2], \"TC average precision\": ap[1]},\n",
    "        engine_train.state.epoch,\n",
    "    )\n",
    "    writer.close()\n",
    "\n",
    "step_scheduler = StepLR(optimizer, step_size=parser_args.steplr_step_size, gamma=parser_args.steplr_gamma)\n",
    "scheduler = create_lr_scheduler_with_warmup(\n",
    "    step_scheduler,\n",
    "    warmup_start_value=parser_args.warmuplr_warmup_start_value,\n",
    "    warmup_end_value=parser_args.warmuplr_warmup_end_value,\n",
    "    warmup_duration=parser_args.warmuplr_warmup_duration,\n",
    ")\n",
    "engine_validate.add_event_handler(Events.EPOCH_COMPLETED, scheduler)\n",
    "\n",
    "earlystopper = EarlyStopping(\n",
    "    patience=parser_args.earlystopping_patience, score_function=lambda x: -x.state.metrics[\"AP\"][1], trainer=engine_train\n",
    ")\n",
    "engine_validate.add_event_handler(Events.EPOCH_COMPLETED, earlystopper)\n",
    "\n",
    "add_tensorboard(engine_train, optimizer, unet, log_dir=parser_args.tensorboard_path)\n",
    "\n",
    "engine_train.run(dataloader_train, max_epochs=parser_args.n_epochs)\n",
    "\n",
    "torch.save(unet.state_dict(), parser_args.model_save_path + \"unet_state.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53960658-9bd8-478f-b0d6-e7dc0b3f1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_compute_fn(y_pred, y_true):\n",
    "    \"\"\"Attached function to the custom ignite metric AveragePrecisionMultiLabel\n",
    "\n",
    "    Args:\n",
    "        y_pred (:obj:`torch.Tensor`): model predictions\n",
    "        y_true (:obj:`torch.Tensor`): ground truths\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: Indicates that sklearn should be installed by the user.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`numpy.array`: average precision vector.\n",
    "                            Of the same length as the number of labels present in the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.metrics import average_precision_score\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\"This metric requires sklearn to be installed.\")\n",
    "\n",
    "    ap = average_precision_score(y_true.numpy(), y_pred.numpy(), None)\n",
    "    return ap\n",
    "\n",
    "\n",
    "# Pylint and Ignite incompatibilities:\n",
    "# pylint: disable=W0612\n",
    "# pylint: disable=W0613\n",
    "\n",
    "\n",
    "def validate_output_transform(x, y, y_pred):\n",
    "    \"\"\"A transform to format the output of the supervised evaluator before calculating the metric\n",
    "\n",
    "    Args:\n",
    "        x (:obj:`torch.Tensor`): the input to the model\n",
    "        y (:obj:`torch.Tensor`): the output of the model\n",
    "        y_pred (:obj:`torch.Tensor`): the ground truth labels\n",
    "\n",
    "    Returns:\n",
    "        (:obj:`torch.Tensor`, :obj:`torch.Tensor`): model predictions and ground truths reformatted\n",
    "    \"\"\"\n",
    "    output = y_pred\n",
    "    labels = y\n",
    "    B, V, C = output.shape\n",
    "    B_labels, V_labels, C_labels = labels.shape\n",
    "    output = output.view(B * V, C)\n",
    "    labels = labels.view(B_labels * V_labels, C_labels)\n",
    "    return output, labels\n",
    "\n",
    "\n",
    "def add_tensorboard(engine_train, optimizer, model, log_dir):\n",
    "    \"\"\"Creates an ignite logger object and adds training elements such as weight and gradient histograms\n",
    "\n",
    "    Args:\n",
    "        engine_train (:obj:`ignite.engine`): the train engine to attach to the logger\n",
    "        optimizer (:obj:`torch.optim`): the model's optimizer\n",
    "        model (:obj:`torch.nn.Module`): the model being trained\n",
    "        log_dir (string): path to where tensorboard data should be saved\n",
    "    \"\"\"\n",
    "    # Create a logger\n",
    "    tb_logger = TensorboardLogger(log_dir=log_dir)\n",
    "\n",
    "    # Attach the logger to the trainer to log training loss at each iteration\n",
    "    tb_logger.attach(\n",
    "        engine_train, log_handler=OutputHandler(tag=\"training\", output_transform=lambda loss: {\"loss\": loss}), event_name=Events.ITERATION_COMPLETED\n",
    "    )\n",
    "\n",
    "    # Attach the logger to the trainer to log optimizer's parameters, e.g. learning rate at each iteration\n",
    "    tb_logger.attach(engine_train, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "    # Attach the logger to the trainer to log model's weights as a histogram after each epoch\n",
    "    tb_logger.attach(engine_train, log_handler=WeightsHistHandler(model), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "    # Attach the logger to the trainer to log model's gradients as a histogram after each epoch\n",
    "    tb_logger.attach(engine_train, log_handler=GradsHistHandler(model), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "    tb_logger.close()\n",
    "\n",
    "\n",
    "def get_dataloaders(parser_args):\n",
    "    \"\"\"Creates the datasets and the corresponding dataloaders\n",
    "\n",
    "    Args:\n",
    "        parser_args (dict): parsed arguments\n",
    "\n",
    "    Returns:\n",
    "        (:obj:`torch.utils.data.dataloader`, :obj:`torch.utils.data.dataloader`): train, validation dataloaders\n",
    "    \"\"\"\n",
    "\n",
    "    path_to_data = parser_args.path_to_data\n",
    "    download = parser_args.download\n",
    "    partition = parser_args.partition\n",
    "    seed = parser_args.seed\n",
    "    means_path = parser_args.means_path\n",
    "    stds_path = parser_args.stds_path\n",
    "\n",
    "    data = ARTCDataset(path=path_to_data, download=download, indices=None, transform_data=None, transform_labels=None)\n",
    "\n",
    "    train_indices, temp = train_test_split(data.indices, train_size=partition[0], random_state=seed)\n",
    "    val_indices, _ = train_test_split(temp, test_size=partition[2] / (partition[1] + partition[2]), random_state=seed)\n",
    "\n",
    "    if (means_path is None) or (stds_path is None):\n",
    "        transform_data_stats = transforms.Compose([ToTensor()])\n",
    "        train_set_stats = ARTCDataset(\n",
    "            path=path_to_data, download=download, indices=train_indices, transform_data=transform_data_stats, transform_labels=None\n",
    "        )\n",
    "        means, stds = stats_extractor(train_set_stats)\n",
    "        np.save(\"./means.npy\", means)\n",
    "        np.save(\"./stds.npy\", stds)\n",
    "    else:\n",
    "        try:\n",
    "            means = np.load(means_path)\n",
    "            stds = np.load(stds_path)\n",
    "        except ValueError:\n",
    "            print(\"No means or stds were provided. Or path names incorrect.\")\n",
    "\n",
    "    transform_data = transforms.Compose([ToTensor(), Permute(), Normalize(mean=means, std=stds)])\n",
    "    transform_labels = transforms.Compose([ToTensor(), Permute()])\n",
    "    train_set = ARTCDataset(\n",
    "        path=path_to_data, download=download, indices=train_indices, transform_data=transform_data, transform_labels=transform_labels\n",
    "    )\n",
    "    validation_set = ARTCDataset(\n",
    "        path=path_to_data, download=download, indices=val_indices, transform_data=transform_data, transform_labels=transform_labels\n",
    "    )\n",
    "\n",
    "    dataloader_train = DataLoader(train_set, batch_size=parser_args.batch_size, shuffle=True, num_workers=12)\n",
    "    dataloader_validation = DataLoader(validation_set, batch_size=parser_args.batch_size, shuffle=False, num_workers=12)\n",
    "    return dataloader_train, dataloader_validation\n",
    "\n",
    "\n",
    "def main(parser_args):\n",
    "    \"\"\"Main function to create trainer engine, add handlers to train and validation engines.\n",
    "    Then runs train engine to perform training and validation.\n",
    "\n",
    "    Args:\n",
    "        parser_args (dict): parsed arguments\n",
    "    \"\"\"\n",
    "    dataloader_train, dataloader_validation = get_dataloaders(parser_args)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    unet = SphericalUNet(parser_args.pooling_class, parser_args.n_pixels, parser_args.depth, parser_args.laplacian_type, parser_args.kernel_size)\n",
    "    unet, device = init_device(parser_args.device, unet)\n",
    "    lr = parser_args.learning_rate\n",
    "    optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
    "\n",
    "    def trainer(engine, batch):\n",
    "        \"\"\"Train Function to define train engine.\n",
    "        Called for every batch of the train engine, for each epoch.\n",
    "\n",
    "        Args:\n",
    "            engine (ignite.engine): train engine\n",
    "            batch (:obj:`torch.utils.data.dataloader`): batch from train dataloader\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.tensor` : train loss for that batch and epoch\n",
    "        \"\"\"\n",
    "        unet.train()\n",
    "        data, labels = batch\n",
    "        labels = labels.to(device)\n",
    "        data = data.to(device)\n",
    "        output = unet(data)\n",
    "\n",
    "        B, V, C = output.shape\n",
    "        B_labels, V_labels, C_labels = labels.shape\n",
    "        output = output.view(B * V, C)\n",
    "        labels = labels.view(B_labels * V_labels, C_labels).max(1)[1]\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    writer = SummaryWriter(parser_args.tensorboard_path)\n",
    "\n",
    "    engine_train = Engine(trainer)\n",
    "\n",
    "    engine_validate = create_supervised_evaluator(\n",
    "        model=unet, metrics={\"AP\": EpochMetric(average_precision_compute_fn)}, device=device, output_transform=validate_output_transform\n",
    "    )\n",
    "\n",
    "    engine_train.add_event_handler(Events.EPOCH_STARTED, lambda x: print(\"Starting Epoch: {}\".format(x.state.epoch)))\n",
    "    engine_train.add_event_handler(Events.ITERATION_COMPLETED, TerminateOnNan())\n",
    "\n",
    "    @engine_train.on(Events.EPOCH_COMPLETED)\n",
    "    def epoch_validation(engine):\n",
    "        \"\"\"Handler to run the validation engine at the end of the train engine's epoch.\n",
    "\n",
    "        Args:\n",
    "            engine (ignite.engine): train engine\n",
    "        \"\"\"\n",
    "        print(\"beginning validation epoch\")\n",
    "        engine_validate.run(dataloader_validation)\n",
    "\n",
    "    reduce_lr_plateau = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=parser_args.reducelronplateau_mode,\n",
    "        factor=parser_args.reducelronplateau_factor,\n",
    "        patience=parser_args.reducelronplateau_patience,\n",
    "    )\n",
    "\n",
    "    @engine_validate.on(Events.EPOCH_COMPLETED)\n",
    "    def update_reduce_on_plateau(engine):\n",
    "        \"\"\"Handler to reduce the learning rate on plateau at the end of the validation engine's epoch\n",
    "\n",
    "        Args:\n",
    "            engine (ignite.engine): validation engine\n",
    "        \"\"\"\n",
    "        ap = engine.state.metrics[\"AP\"]\n",
    "        mean_average_precision = np.mean(ap[1:])\n",
    "        reduce_lr_plateau.step(mean_average_precision)\n",
    "\n",
    "    @engine_validate.on(Events.EPOCH_COMPLETED)\n",
    "    def save_epoch_results(engine):\n",
    "        \"\"\"Handler to save the metrics at the end of the validation engine's epoch\n",
    "\n",
    "        Args:\n",
    "            engine (ignite.engine): validation engine\n",
    "        \"\"\"\n",
    "        ap = engine.state.metrics[\"AP\"]\n",
    "        mean_average_precision = np.mean(ap[1:])\n",
    "        print(\"Average precisions:\", ap)\n",
    "        print(\"mAP:\", mean_average_precision)\n",
    "        writer.add_scalars(\n",
    "            \"metrics\",\n",
    "            {\"mean average precision (AR+TC)\": mean_average_precision, \"AR average precision\": ap[2], \"TC average precision\": ap[1]},\n",
    "            engine_train.state.epoch,\n",
    "        )\n",
    "        writer.close()\n",
    "\n",
    "    step_scheduler = StepLR(optimizer, step_size=parser_args.steplr_step_size, gamma=parser_args.steplr_gamma)\n",
    "    scheduler = create_lr_scheduler_with_warmup(\n",
    "        step_scheduler,\n",
    "        warmup_start_value=parser_args.warmuplr_warmup_start_value,\n",
    "        warmup_end_value=parser_args.warmuplr_warmup_end_value,\n",
    "        warmup_duration=parser_args.warmuplr_warmup_duration,\n",
    "    )\n",
    "    engine_validate.add_event_handler(Events.EPOCH_COMPLETED, scheduler)\n",
    "\n",
    "    earlystopper = EarlyStopping(\n",
    "        patience=parser_args.earlystopping_patience, score_function=lambda x: -x.state.metrics[\"AP\"][1], trainer=engine_train\n",
    "    )\n",
    "    engine_validate.add_event_handler(Events.EPOCH_COMPLETED, earlystopper)\n",
    "\n",
    "    add_tensorboard(engine_train, optimizer, unet, log_dir=parser_args.tensorboard_path)\n",
    "\n",
    "    engine_train.run(dataloader_train, max_epochs=parser_args.n_epochs)\n",
    "\n",
    "    torch.save(unet.state_dict(), parser_args.model_save_path + \"unet_state.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PARSER_ARGS = parse_config(create_parser())\n",
    "    main(PARSER_ARGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda100",
   "language": "python",
   "name": "cuda100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
